---
format: 
  pdf:
    documentclass: article
    classoption: ["a4paper", "12pt", "fleqn"]
    geometry: top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm
    number-sections: true
    number-depth: 8
    toc: false  # Désactiver le sommaire automatique
header-includes: |
  \usepackage{hyperref}  % Liens cliquables
  \hypersetup{hidelinks}  % Désactive complètement la mise en couleur des liens
editor: 
  markdown: 
    wrap: 72
---

\begin{titlepage}
    \begin{center}
        {\LARGE \textbf{Séries temporelles univariées}}\\
        \vspace{0.5cm}
        {\Large M1 ECAP -- TD4 -- Année 2024/2025}\\
        
        \vspace{2cm}
        
          {\Large \textbf{TD4 : Analyse de la non-stationnarité et racines unitaires}}\\
        \vspace{0.5cm}
        \textit{Responsable d'enseignement : Benoît SÉVI}\\
        \href{mailto:benoit.sevi@univ-nantes.fr}{benoit.sevi@univ-nantes.fr}\\
        
        \vspace{1.5cm}
        
        {\large \textbf{DAËRON Djayan, HERVÉ Isaline}}
        
        \vfill
        
        {\large \today}
        
    \end{center}
\end{titlepage}
\begingroup
\hypersetup{linkcolor=black}
\tableofcontents
\endgroup

\newpage

# Chargement des packages nécessaires

```{r}
#| output: false

library(tseries)
library(forecast)
library(FinTS)
library(Metrics)
library(TSA)

library(zoo) # utilisée
library(MASS) # utilisée
```

\newpage

# Exercice 1 : Chocs permanents vs chocs transitoires

## Choc de valeur 20

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- 20
choc_moment <- 100
```

### Simulation AR(1) avec choc 20

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, simulate_AR1, n = n, choc_moment = choc_moment, choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], type = "l", col = "darkgreen", ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur 20")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", legend = c("phi = 0.5", "phi = 0.9", "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),lwd=2, cex = 0.5)
```

## Choc de valeur 40

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- 40
choc_moment <- 100
```

### Simulation AR(1) avec choc 40

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, simulate_AR1, n = n, choc_moment = choc_moment, choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], type = "l", col = "darkgreen", ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur 40")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", legend = c("phi = 0.5", "phi = 0.9", "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),lwd=2, cex = 0.5)
```

## Choc de valeur -20

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- -20
choc_moment <- 100
```

### Simulation AR(1) avec choc -20

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, simulate_AR1, n = n, choc_moment = choc_moment, choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], type = "l", col = "darkgreen", ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur -20")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", legend = c("phi = 0.5", "phi = 0.9", "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),lwd=2, cex = 0.5)
```

## Choc de valeur -40

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- -40
choc_moment <- 100
```

### Simulation AR(1) avec choc -40

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, simulate_AR1, n = n, choc_moment = choc_moment, choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], type = "l", col = "darkgreen", ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur -40")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", legend = c("phi = 0.5", "phi = 0.9", "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),lwd=2, cex = 0.5)
```

## Interprétation des simulations AR(1)

Lorsque $\phi_1 = 0.5$ ou $\phi_1 = 0.9$, nous constatons que les chocs (positifs ou négatifs) sont transitoires. L'impact diminue progressivement et la série revient vers son niveau initial. Plus $\phi$ est proche de 1, plus la dissipation du choc semble lente.

D'autre part, lorsque $\phi_1 = 1$ (racine unitaire), les chocs impactent durablement la série. La série suit une marche aléatoire et ne revient pas à son niveau précédent.

Alors, nous pouvons voir que la présence d'une racine unitaire donnera une absence de stationnarité et de tendance à la moyenne, mais une sensibilité accrue aux chocs, qui affecteront la série de manière permanente.


\newpage

# Exercice 2 : TS vs DS : simulation de processus

## Code

### Paramètres
```{r}
set.seed(123)  # Fixer la graine pour reproductibilité
T <- 200       # Nombre d'observations
t <- 1:T       # Temps
```

### Définition des variances
```{r}
set.seed(123)

variances <- c(1/4, 1/2, 1)  
names(variances) <- c("Var = 1/4", "Var = 1/2", "Var = 1")
```

### Création du graphique pour chaque variance
```{r}
set.seed(123)

for (var in variances) {
  # Génération des bruits epsilon_t
  epsilon <- rnorm(T, mean = 0, sd = sqrt(var))

  # Processus TS : Y_t = 0.2t + epsilon_t
  Y_TS <- 0.2 * t + epsilon

  # Processus DS (UR) : Y_t = 0.2 + Y_t-1 + epsilon_t
  Y_DS <- numeric(T)
  Y_DS[1] <- 0  # Condition initiale
  for (i in 2:T) {
    Y_DS[i] <- 0.2 + Y_DS[i-1] + epsilon[i]
  }
  
  # Tracé du graphique
  plot(t, 
       Y_TS, 
       type = "l", 
       col = "blue", 
       lwd = 2, 
       ylim = range(c(Y_TS, Y_DS)),
       main = paste("Comparaison TS vs DS -", 
                    names(variances)[which(variances == var)]),
       xlab = "Temps", ylab = "Valeur")
  lines(t, Y_DS, col = "red", lwd = 2)
  legend("topleft", legend = c("TS: 0.2_t + ε_t", 
                               "DS: 0.2 + Y_{t-1} + ε_t"),
         col = c("blue", "red"), lwd = 2, bty = "n")
}

```


## Interprétation des graphiques

Les trois graphiques montrent l'évolution des deux processus temporels (TS en bleu et DS en rouge) pour différentes valeurs de variance des erreurs ($\varepsilon$).

\noindent \textbf{1. Comportement général}


- **TS (Trend Stationary - en bleu)** : suit une tendance linéaire ($0.2t$) avec des fluctuations autour de cette tendance.

- **DS (Difference Stationary - en rouge)** : suit un processus de marche aléatoire avec drift ($0.2 + Y_{t-1} + \varepsilon_t$), ce qui lui permet de dériver plus librement au fil du temps.

\noindent \textbf{2. Effet de la variance de $\varepsilon$}

- **Première courbe (Var = 1/4)** :

  - Les séries TS et DS restent proches, avec des écarts modérés.
  
  - La variance faible limite la dispersion des valeurs.

- **Deuxième courbe (Var = 1/2)** :

  - L’écart entre TS et DS devient plus visible.
  
  - Le processus DS commence à montrer des écarts plus importants par rapport à TS.

- **Troisième courbe (Var = 1)** :

  - La série DS (rouge) devient beaucoup plus volatile.
  
  - Contrairement à TS qui reste centré autour de sa tendance, DS prend une trajectoire plus erratique.

\noindent \textbf{3. Conclusion}

- Le processus **TS** reste stable autour de sa tendance, quelle que soit la variance.

- Le processus **DS** montre un comportement de marche aléatoire, avec une dérive qui devient plus marquée à mesure que la variance augmente.

- Lorsque la variance est élevée, **DS devient beaucoup plus instable**, ce qui est typique d’un processus à racine unitaire.

**Interprétation économique/statistique** :

- Un processus **Trend Stationary (TS)** revient vers une tendance prévisible après un choc (stationnarité autour d’une tendance).

- Un processus **Difference Stationary (DS)** ne revient pas nécessairement à une tendance après un choc, ce qui signifie qu'il peut dériver sur le long terme.

- En pratique, les séries macroéconomiques comme le PIB ou l’inflation sont souvent DS plutôt que TS, ce qui implique que leurs chocs ont des effets durables.


# Exercice 3 : Régressions fallacieuses

## Créer une fonction pour simuler

```{r}
simu_reg_fal <- function(n_simul = 5000, n_obs = 200, alpha = 0.05) {
  # Mettre une graine
  set.seed(123)
  
  # Initialisation du vecteur pour stocker les p-values
  p_values <- numeric(n_simul)
  
  # Boucle de simulation
  for (i in 1:n_simul) {
    # Génération de deux marches aléatoires indépendantes
    X <- cumsum(rnorm(n_obs))
    Y <- cumsum(rnorm(n_obs))
    
    # Régression Y_t ~ X_t
    model <- lm(Y ~ X)
    
    # Récupération de la p-value du test de Student sur β1
    p_values[i] <- summary(model)$coefficients[2, 4]  
  }
  
  # Calcul du pourcentage de rejets de H0 (p-value < alpha)
  rejet_pourcentage <- mean(p_values < alpha) * 100
  
  return(rejet_pourcentage)
}
```

## Simuler la série

```{r}
simu_reg_fal(n_simul = 5000, n_obs = 200, alpha = 0.05)
```

## Modifier les paramètres

### Modifier le nombre de séries générées

```{r}
# Base (n_simul = 5000)
simu_reg_fal(n_simul = 5000, n_obs = 200, alpha = 0.05)

# Diminuer (n_simul = 1000)
simu_reg_fal(n_simul = 1000, n_obs = 200, alpha = 0.05)

# Augmenter (n_simul = 10000)
simu_reg_fal(n_simul = 10000, n_obs = 200, alpha = 0.05)
```

### Modifier le nombre d'observation

```{r}
# Base (n_obs = 200)
simu_reg_fal(n_simul = 5000, n_obs = 200, alpha = 0.05)

# Diminuer (n_obs = 50)
simu_reg_fal(n_simul = 5000, n_obs = 50, alpha = 0.05)

# Augmenter (n_obs = 1000)
simu_reg_fal(n_simul = 5000, n_obs = 1000, alpha = 0.05)
```


## Interprétation

Nous obtenons ici un taux de rejet de H0 de 83.52%, donc même si les séries sont indépendantes, nous rejetons H0 très souvent. Cela est du au fait que les marches aléatoires sont non stationnaires et à la présence d'une racine unitaire.
Alors, la régression entre deux marches aléatoires peut détecter des relations factices.

Nous pouvons voir qu'en simulant plus ou moins de séries, le taux de rejet reste globalement stable, entre 82 ou 83% pour notre cas.

Cependant, en faisant varier le nombre d'observation, cela influence davantage les résultats.
D'une part, en diminuant $n$, nous constatons que le taux de rejet diminue.
D'autre part, en augmentant le nombre d'observations, le taux de rejet augmente, car la présence de tendances fallacieuses sera plus élevée.

\newpage


# Exercice 4 : Distribution de la statistique de test de DF pour le modèle sans constante ni tendance via la méthode de Monte Carlo

## Calcul des valeurs critiques pour le test DF

### Définition des paramètres et de la fonction

```{r}
set.seed(123)

n_simulations <- 10000
n_obs <- 100
t_stats <- numeric(n_simulations)

generate_random_walk <- function(n) {
  cumsum(rnorm(n)) 
}

for (i in 1:n_simulations) {
  Yt <- generate_random_walk(n_obs)
  
  dYt <- diff(Yt) 
  Yt_lag <- Yt[-n_obs]
  
  model <- lm(dYt ~ Yt_lag - 1)
  t_stats[i] <- summary(model)$coefficients[1, 3]
}

quantiles <- quantile(t_stats, c(0.10, 0.05, 0.01))
```

### Représentation graphique de la distribution des t-statistiques

```{r}
plot_monte_carlo_distribution <- function(t_stats, quantiles) {
  # Tracer l'histogramme en fond discret
  hist(t_stats, breaks = 50, probability = TRUE, col = adjustcolor("lightgray", alpha.f = 0.5), border = NA,
       main = "Distribution Monte Carlo des t-stat de Dickey-Fuller",
       xlab = "Valeur de la statistique t", ylab = "Densité")

  # Estimer la densité
  dens <- density(t_stats, adjust = 1.5)

  # Tracer la courbe de densité
  lines(dens, col = "royalblue", lwd = 3)

  # Fonction pour créer des hachures sous la courbe de densité
  hachurer_zone <- function(quantile_value, color, density, angle, lwd) {
    x_vals <- dens$x[dens$x <= quantile_value]  # Prendre seulement les x <= quantile
    y_vals <- dens$y[dens$x <= quantile_value]  # Correspondance des y

    polygon(c(x_vals, rev(x_vals)), c(rep(0, length(x_vals)), rev(y_vals)),
            col = adjustcolor(color, alpha.f = 0.3), border = NA, density = density, angle = angle, lwd = lwd)
  }

  # Appliquer les hachures aux zones avant les quantiles avec des largeurs de traits plus grosses
  hachurer_zone(quantiles[1], "orange", density = 10, angle = 45, lwd = 2)   # 10%
  hachurer_zone(quantiles[2], "red", density = 15, angle = 45, lwd = 3)      # 5%
  hachurer_zone(quantiles[3], "darkred", density = 20, angle = 45, lwd = 4)  # 1%

  # Ajouter des lignes verticales pour les quantiles
  abline(v = quantiles, col = c("orange", "red", "darkred"), lwd = 3, lty = 2)

  # Ajouter une légende repositionnée à l'extérieur
  par(xpd = TRUE)  # Permet de dessiner en dehors de la zone du graphique
  legend("topright", legend = c("Densité estimée", "Quantile 10% (hachuré)", "Quantile 5% (hachuré)", "Quantile 1% (hachuré)"),
         col = c("royalblue", "orange", "red", "darkred"), lwd = 3, lty = c(1, NA, NA, NA),
         fill = c(NA, adjustcolor("orange", alpha.f = 0.3), adjustcolor("red", alpha.f = 0.3), adjustcolor("darkred", alpha.f = 0.3)),
         density = c(NA, 10, 15, 20), angle = 45, cex = 0.8, bg = "white", inset = c(-0.2, 0))
  par(xpd = FALSE)  # Réinitialisation pour éviter d'affecter les autres tracés
}

plot_monte_carlo_distribution(t_stats, quantiles)


```

### Affichage des valeurs critiques

```{r}
cat("Valeurs critiques obtenues :\n")
print(quantiles)
```

Nous pouvons voir que les valeurs obtenues sont très proches des valeurs théoriques, étant pour 1%, -2,60, pour 5%, -1,95 et pour 10%, -1,61.

## Analyse avec constante

### $\Delta_0 = 5$

```{r}
set.seed(123)

n_simulations <- 10000
n_obs <- 100

# Fonction pour générer une marche aléatoire avec constante
generate_random_walk_avec_const <- function(n, delta0) {
  Yt <- numeric(n)
  Yt[1] <- delta0  # Initialisation avec la constante
  for (t in 2:n) {
    Yt[t] <- Yt[t-1] + rnorm(1)
  }
  return(Yt)
}

# Fonction pour exécuter la simulation et afficher les résultats
simulation_cst <- function(delta0) {
  set.seed(123)  # Réinitialisation de la graine pour reproductibilité
  
  t_stats <- numeric(n_simulations)  # Réinitialisation à chaque appel
  
  for (i in 1:n_simulations) {
    Yt <- generate_random_walk_avec_const(n_obs, delta0)
    
    dYt <- diff(Yt)
    Yt_lag <- Yt[-n_obs]
    
    model <- lm(dYt ~ Yt_lag)  # Régression avec constante par défaut
    t_stats[i] <- summary(model)$coefficients[2, 3]  # Récupération du t-stat
  }
  
  # Calcul des quantiles (valeurs critiques)
  quantiles <- quantile(t_stats, c(0.10, 0.05, 0.01))
  
  plot_monte_carlo_distribution(t_stats, quantiles)
  
  # Affichage des valeurs critiques
  cat("Valeurs critiques obtenues pour δ0 =", delta0, ":\n")
  print(quantiles)
}


simulation_cst(delta0 = 5)
simulation_cst(delta0 = 10)

```

La valeur de la constante $\delta_0$ n'a pas d'impact significatif sur les résultats du test de Dickey-Fuller dans ce contexte. Peu importe si la constante est 5, 10, ou une autre valeur, la distribution des statistiques de test reste la même. Cela suggère que le test est robuste vis-à-vis de la valeur de la constante, et qu'il est davantage influencé par les autres caractéristiques du modèle plutôt que par la valeur spécifique de cette constante.


# Exercice 5 

## Import des données

```{r}
load("data/GDP-US-1990-2023.RData")
```

## Création de la ts pour la période globale

```{r}
pib_pg <- ts(df_Y_us$PIB, start = c(1990, 1), frequency = 4)  
```

## Création de la ts pour la sous période 1990-2019

```{r}
pib_sp <- window(pib_pg, start = c(1990, 1), end = c(2019, 4))
```

## 1. Représentation graphique

Les périodes de récession ont ici été obtenues via le NBER (National Bureau of Economic Research). Nous retrouvons les périodes de juillet 1990 à mars 1991, de mars 2001 à novembre 2001, et de décembre 2007 à juin 2009.

```{r}
library(ggplot2)
library(scales)

pib_sp_df <- data.frame(date = seq(from =
              as.Date("1990-01-01"),
              by = "quarter", 
              length.out = length(pib_sp)),
              PIB = as.numeric(pib_sp))

recessions <- data.frame(
  xmin = as.Date(c("1990-07-01", "2001-03-01", "2007-12-01")),
  xmax = as.Date(c("1991-03-01", "2001-11-01", "2009-06-01"))
)

ggplot(pib_sp_df, aes(x = date, y = PIB)) +
  geom_line(color = "darkred", linewidth = 1) + 
  geom_rect(data = recessions, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
            fill = "darkgray", alpha = 0.5, inherit.aes = FALSE) +
  scale_x_date(breaks = seq(as.Date("1990-01-01"), as.Date("2019-12-31"), by = "4 years"), 
               labels = date_format("%Y")) +
  labs(title = "Évolution du PIB US (1990-2019) avec récessions",
       x = "Année", y = "PIB (Milliards $)") +
  theme_minimal()
```

## 2. ACF et PACF

```{r}

```

## 3. Procédure de racine unitaire

### Test de Dickey-Fuller simple 

```{r}

```

### Test de Dickey-Fuller augmenté

```{r}

```

### Conclusion


## 4. Test de stationnarité de KPSS

```{r}

```

### Conclusion

## 5. Analyse sur période globale

### Représentation graphique

```{r}
pib_pg_df <- data.frame(date = seq(from =
              as.Date("1990-01-01"),
              by = "quarter", 
              length.out = length(pib_pg)),
              PIB = as.numeric(pib_pg))

ggplot(pib_pg_df, aes(x = date, y = PIB)) +
  geom_line(color = "darkviolet", linewidth = 1) + 
  geom_rect(data = recessions, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
            fill = "darkgray", alpha = 0.5, inherit.aes = FALSE) +
  scale_x_date(breaks = seq(as.Date("1990-01-01"), as.Date("2023-12-31"), by = "3 years"), 
               labels = date_format("%Y")) +
  labs(title = "Évolution du PIB US (1990-2023) avec récessions",
       x = "Année", y = "PIB (Milliards $)") +
  theme_minimal()
```

### ACF, PACF de la série

```{r}

```

### Tests de DF 

#### Test Dickey-Fuller simple

```{r}

```

#### Test Dickey-Fuller augmenté

```{r}

```

### Test de stationnarité KPSS

```{r}

```

### Conclusion

