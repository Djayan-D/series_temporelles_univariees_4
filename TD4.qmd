---
format: 
  pdf:
    documentclass: article
    classoption: ["a4paper", "12pt", "fleqn"]
    geometry: top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm
    number-sections: true
    number-depth: 8
    toc: false  # Désactiver le sommaire automatique
header-includes: |
  \usepackage{hyperref}  % Liens cliquables
  \hypersetup{hidelinks}  % Désactive complètement la mise en couleur des liens
editor: 
  markdown: 
    wrap: 72
---

\begin{titlepage}
    \begin{center}
        {\LARGE \textbf{Séries temporelles univariées}}\\
        \vspace{0.5cm}
        {\Large M1 ECAP -- TD4 -- Année 2024/2025}\\
        
        \vspace{2cm}
        
          {\Large \textbf{TD4 : Analyse de la non-stationnarité et racines unitaires}}\\
        \vspace{0.5cm}
        \textit{Responsable d'enseignement : Benoît SÉVI}\\
        \href{mailto:benoit.sevi@univ-nantes.fr}{benoit.sevi@univ-nantes.fr}\\
        
        \vspace{1.5cm}
        
        {\large \textbf{DAËRON Djayan, HERVÉ Isaline}}
        
        \vfill
        
        {\large \today}
        
    \end{center}
\end{titlepage}
\begingroup
\hypersetup{linkcolor=black}
\tableofcontents
\endgroup

\newpage

# Chargement des packages nécessaires

```{r}
#| output: false

library(tseries) 
library(forecast)
library(FinTS)
library(Metrics)
library(TSA)

library(zoo) # utilisée
library(MASS) # utilisée
library(urca) # utilisée
library(scales) # utilisée
library(ggplot2) # utilisée
```

\newpage

# Exercice 1 : Chocs permanents vs chocs transitoires

## Choc de valeur 20

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- 20
choc_moment <- 100
```

### Simulation AR(1) avec choc 20

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, 
                 simulate_AR1, 
                 n = n, 
                 choc_moment = choc_moment, 
                 choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], 
     type = "l", 
     col = "darkgreen", 
     ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur 20")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", 
       legend = c("phi = 0.5", 
                  "phi = 0.9", 
                  "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),
       lwd=2, 
       cex = 0.5)
```

## Choc de valeur 40

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- 40
choc_moment <- 100
```

### Simulation AR(1) avec choc 40

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, 
                 simulate_AR1, 
                 n = n, 
                 choc_moment = choc_moment, 
                 choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], 
     type = "l", 
     col = "darkgreen", 
     ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur 40")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", 
       legend = c("phi = 0.5", 
                  "phi = 0.9", 
                  "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),
       lwd=2, 
       cex = 0.5)
```

## Choc de valeur -20

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- -20
choc_moment <- 100
```

### Simulation AR(1) avec choc -20

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, 
                 simulate_AR1, 
                 n = n, 
                 choc_moment = choc_moment, 
                 choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], 
     type = "l", 
     col = "darkgreen", 
     ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur -20")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", 
       legend = c("phi = 0.5", 
                  "phi = 0.9", 
                  "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),
       lwd=2, 
       cex = 0.5)
```

## Choc de valeur -40

### Définition des paramètres

```{r}
n <- 200
phi_values <- c(0.5, 0.9, 1) 
choc_valeur <- -40
choc_moment <- 100
```

### Simulation AR(1) avec choc -40

```{r}
set.seed(123)
simulate_AR1 <- function(phi, n, choc_moment, choc_valeur) {
  e <- rnorm(n)
  y <- numeric(n)
  for (t in 2:n) {
    y[t] <- phi * y[t-1] + e[t]
    if (t == choc_moment) y[t] <- y[t] + choc_valeur
  }
  return(y)
}

series <- lapply(phi_values, 
                 simulate_AR1, 
                 n = n, 
                 choc_moment = choc_moment, 
                 choc_valeur = choc_valeur)
```

### Visualisation graphique

```{r}
plot(series[[1]], 
     type = "l", 
     col = "darkgreen", 
     ylim = range(unlist(series)),
     ylab = "Valeur", 
     xlab = "Temps", 
     main = "AR(1) avec choc de valeur -40")
lines(series[[2]], col = "royalblue")
lines(series[[3]], col = "darkred")
legend("topleft", 
       legend = c("phi = 0.5", 
                  "phi = 0.9", 
                  "phi = 1 (racine unitaire)"),
       col = c("darkgreen", "royalblue", "darkred"),
       lwd=2, 
       cex = 0.5)
```

## Interprétation des simulations AR(1)

Lorsque $\phi_1 = 0.5$ ou $\phi_1 = 0.9$, nous constatons que les chocs (positifs ou négatifs) sont transitoires. L'impact diminue progressivement et la série revient vers son niveau initial. Plus $\phi$ est proche de 1, plus la dissipation du choc semble lente.

D'autre part, lorsque $\phi_1 = 1$ (racine unitaire), les chocs impactent durablement la série. La série suit une marche aléatoire et ne revient pas à son niveau précédent.

Alors, nous pouvons voir que la présence d'une racine unitaire donnera une absence de stationnarité et de tendance à la moyenne, mais une sensibilité accrue aux chocs, qui affecteront la série de manière permanente.


\newpage

# Exercice 2 : TS vs DS : simulation de processus

## Code

### Paramètres
```{r}
set.seed(123)  # Fixer la graine pour reproductibilité
T <- 200       # Nombre d'observations
t <- 1:T       # Temps
```

### Définition des variances
```{r}
set.seed(123)

variances <- c(1/4, 1/2, 1)  
names(variances) <- c("Var = 1/4", "Var = 1/2", "Var = 1")
```

### Création du graphique pour chaque variance
```{r}
set.seed(123)

for (var in variances) {
  # Génération des bruits epsilon_t
  epsilon <- rnorm(T, mean = 0, sd = sqrt(var))

  # Processus TS : Y_t = 0.2t + epsilon_t
  Y_TS <- 0.2 * t + epsilon

  # Processus DS (UR) : Y_t = 0.2 + Y_t-1 + epsilon_t
  Y_DS <- numeric(T)
  Y_DS[1] <- 0  # Condition initiale
  for (i in 2:T) {
    Y_DS[i] <- 0.2 + Y_DS[i-1] + epsilon[i]
  }
  
  # Tracé du graphique
  plot(t, 
       Y_TS, 
       type = "l", 
       col = "blue", 
       lwd = 2, 
       ylim = range(c(Y_TS, Y_DS)),
       main = paste("Comparaison TS vs DS -", 
                    names(variances)[which(variances == var)]),
       xlab = "Temps", ylab = "Valeur")
  lines(t, Y_DS, col = "red", lwd = 2)
  legend("topleft", legend = c("TS: 0.2_t + ε_t", 
                               "DS: 0.2 + Y_{t-1} + ε_t"),
         col = c("blue", "red"), lwd = 2, bty = "n")
}

```


## Interprétation des graphiques

Les trois graphiques montrent l'évolution des deux processus temporels (TS en bleu et DS en rouge) pour différentes valeurs de variance des erreurs ($\varepsilon$).

\noindent \textbf{1. Comportement général}


- **TS (Trend Stationary - en bleu)** : suit une tendance linéaire ($0.2t$) avec des fluctuations autour de cette tendance.

- **DS (Difference Stationary - en rouge)** : suit un processus de marche aléatoire avec drift ($0.2 + Y_{t-1} + \varepsilon_t$), ce qui lui permet de dériver plus librement au fil du temps.

\noindent \textbf{2. Effet de la variance de $\varepsilon$}

- **Première courbe (Var = 1/4)** :

  - Les séries TS et DS restent proches, avec des écarts modérés.
  
  - La variance faible limite la dispersion des valeurs.

- **Deuxième courbe (Var = 1/2)** :

  - L’écart entre TS et DS devient plus visible.
  
  - Le processus DS commence à montrer des écarts plus importants par rapport à TS.

- **Troisième courbe (Var = 1)** :

  - La série DS (rouge) devient beaucoup plus volatile.
  
  - Contrairement à TS qui reste centré autour de sa tendance, DS prend une trajectoire plus erratique.

\noindent \textbf{3. Conclusion}

- Le processus **TS** reste stable autour de sa tendance, quelle que soit la variance.

- Le processus **DS** montre un comportement de marche aléatoire, avec une dérive qui devient plus marquée à mesure que la variance augmente.

- Lorsque la variance est élevée, **DS devient beaucoup plus instable**, ce qui est typique d’un processus à racine unitaire.

**Interprétation économique/statistique** :

- Un processus **Trend Stationary (TS)** revient vers une tendance prévisible après un choc (stationnarité autour d’une tendance).

- Un processus **Difference Stationary (DS)** ne revient pas nécessairement à une tendance après un choc, ce qui signifie qu'il peut dériver sur le long terme.

- En pratique, les séries macroéconomiques comme le PIB ou l’inflation sont souvent DS plutôt que TS, ce qui implique que leurs chocs ont des effets durables.


# Exercice 3 : Régressions fallacieuses

## Créer une fonction pour simuler

```{r}
simu_reg_fal <- function(n_simul = 5000, n_obs = 200, alpha = 0.05) {
  # Mettre une graine
  set.seed(123)
  
  # Initialisation du vecteur pour stocker les p-values
  p_values <- numeric(n_simul)
  
  # Boucle de simulation
  for (i in 1:n_simul) {
    # Génération de deux marches aléatoires indépendantes
    X <- cumsum(rnorm(n_obs))
    Y <- cumsum(rnorm(n_obs))
    
    # Régression Y_t ~ X_t
    model <- lm(Y ~ X)
    
    # Récupération de la p-value du test de Student sur β1
    p_values[i] <- summary(model)$coefficients[2, 4]  
  }
  
  # Calcul du pourcentage de rejets de H0 (p-value < alpha)
  rejet_pourcentage <- mean(p_values < alpha) * 100
  
  return(rejet_pourcentage)
}
```

## Simuler la série

```{r}
simu_reg_fal(n_simul = 5000, n_obs = 200, alpha = 0.05)
```

## Modifier les paramètres

### Modifier le nombre de séries générées

```{r}
# Base (n_simul = 5000)
simu_reg_fal(n_simul = 5000, n_obs = 200, alpha = 0.05)

# Diminuer (n_simul = 1000)
simu_reg_fal(n_simul = 1000, n_obs = 200, alpha = 0.05)

# Augmenter (n_simul = 10000)
simu_reg_fal(n_simul = 10000, n_obs = 200, alpha = 0.05)
```

### Modifier le nombre d'observation

```{r}
# Base (n_obs = 200)
simu_reg_fal(n_simul = 5000, n_obs = 200, alpha = 0.05)

# Diminuer (n_obs = 50)
simu_reg_fal(n_simul = 5000, n_obs = 50, alpha = 0.05)

# Augmenter (n_obs = 1000)
simu_reg_fal(n_simul = 5000, n_obs = 1000, alpha = 0.05)
```


## Interprétation

Nous obtenons ici un taux de rejet de H0 de 83.52%, donc même si les séries sont indépendantes, nous rejetons H0 très souvent. Cela est du au fait que les marches aléatoires sont non stationnaires et à la présence d'une racine unitaire.
Alors, la régression entre deux marches aléatoires peut détecter des relations factices.

Nous pouvons voir qu'en simulant plus ou moins de séries, le taux de rejet reste globalement stable, entre 82 ou 83% pour notre cas.

Cependant, en faisant varier le nombre d'observation, cela influence davantage les résultats.
D'une part, en diminuant $n$, nous constatons que le taux de rejet diminue.
D'autre part, en augmentant le nombre d'observations, le taux de rejet augmente, car la présence de tendances fallacieuses sera plus élevée.

\newpage


# Exercice 4 : Distribution de la statistique de test de DF pour le modèle sans constante ni tendance via la méthode de Monte Carlo

## Calcul des valeurs critiques pour le test DF

### Définition des paramètres et de la fonction

```{r}
set.seed(123)

n_simulations <- 10000
n_obs <- 100
t_stats <- numeric(n_simulations)

generate_random_walk <- function(n) {
  cumsum(rnorm(n)) 
}

for (i in 1:n_simulations) {
  Yt <- generate_random_walk(n_obs)
  
  dYt <- diff(Yt) 
  Yt_lag <- Yt[-n_obs]
  
  model <- lm(dYt ~ Yt_lag - 1)
  t_stats[i] <- summary(model)$coefficients[1, 3]
}

quantiles <- quantile(t_stats, c(0.10, 0.05, 0.01))
```

### Représentation graphique de la distribution des t-statistiques

```{r}
plot_monte_carlo_distribution <- function(t_stats, quantiles) {
  # Tracer l'histogramme en fond discret
  hist(t_stats, breaks = 50, 
       probability = TRUE, 
       col = adjustcolor("lightgray", alpha.f = 0.5), 
       border = NA,
       main = "Distribution Monte Carlo des t-stat de Dickey-Fuller",
       xlab = "Valeur de la statistique t", ylab = "Densité")

  # Estimer la densité
  dens <- density(t_stats, adjust = 1.5)

  # Tracer la courbe de densité
  lines(dens, col = "royalblue", lwd = 3)

  # Fonction pour créer des hachures sous la courbe de densité
  hachurer_zone <- function(quantile_value, color, density, angle, lwd) {
    x_vals <- dens$x[dens$x <= quantile_value]
    y_vals <- dens$y[dens$x <= quantile_value]

    polygon(c(x_vals, rev(x_vals)), 
            c(rep(0, length(x_vals)), rev(y_vals)),
            col = adjustcolor(color, alpha.f = 0.3), 
            border = NA, 
            density = density, 
            angle = angle, 
            lwd = lwd)
  }

  # Appliquer les hachures aux zones avant les quantiles
  hachurer_zone(quantiles[1], 
                "orange", 
                density = 10, 
                angle = 45, 
                lwd = 2)
  hachurer_zone(quantiles[2], 
                "red", 
                density = 15, 
                angle = 45, 
                lwd = 3)
  hachurer_zone(quantiles[3], 
                "darkred", 
                density = 20, 
                angle = 45, 
                lwd = 4)

  # Ajouter des lignes verticales pour les quantiles
  abline(v = quantiles, 
         col = c("orange", "red", "darkred"), 
         lwd = 3, lty = 2)

  # Ajouter une légende
  par(xpd = TRUE)  # Permet de dessiner en dehors de la zone du graphique
  legend("topright", 
         legend = c("Densité estimée", "Quantile 10% (hachuré)", 
                    "Quantile 5% (hachuré)", "Quantile 1% (hachuré)"),
         col = c("royalblue", "orange", "red", "darkred"), 
         lwd = 3, 
         lty = c(1, NA, NA, NA),
         fill = c(NA, adjustcolor("orange", alpha.f = 0.3), 
                  adjustcolor("red", alpha.f = 0.3), 
                  adjustcolor("darkred", alpha.f = 0.3)),
         density = c(NA, 10, 15, 20), 
         angle = 45, 
         cex = 0.8, 
         bg = "white", 
         inset = c(-0.2, 0))
  par(xpd = FALSE)  # Réinitialisation pour éviter d'affecter les autres tracés
}

plot_monte_carlo_distribution(t_stats, quantiles)
```

### Affichage des valeurs critiques

```{r}
print(quantiles)
```

Nous pouvons voir que les valeurs obtenues sont très proches des valeurs théoriques, étant pour 1%, -2,60, pour 5%, -1,95 et pour 10%, -1,61.

## Analyse avec constante

```{r}
set.seed(123)

n_simulations <- 10000
n_obs <- 100

# Fonction pour générer une marche aléatoire avec constante
generate_random_walk_avec_const <- function(n, delta0) {
  Yt <- numeric(n)
  Yt[1] <- delta0  # Initialisation avec la constante
  for (t in 2:n) {
    Yt[t] <- Yt[t-1] + rnorm(1)
  }
  return(Yt)
}

# Fonction pour exécuter la simulation et afficher les résultats
simulation_cst <- function(delta0) {
  set.seed(123)  # Réinitialisation de la graine pour reproductibilité
  
  t_stats <- numeric(n_simulations)  # Réinitialisation à chaque appel
  
  for (i in 1:n_simulations) {
    Yt <- generate_random_walk_avec_const(n_obs, delta0)
    
    dYt <- diff(Yt)
    Yt_lag <- Yt[-n_obs]
    
    model <- lm(dYt ~ Yt_lag)  # Régression avec constante par défaut
    t_stats[i] <- summary(model)$coefficients[2, 3]  # Récupération du t-stat
  }
  
  # Calcul des quantiles (valeurs critiques)
  quantiles <- quantile(t_stats, c(0.10, 0.05, 0.01))
  
  plot_monte_carlo_distribution(t_stats, quantiles)
  
  # Affichage des valeurs critiques
  cat("Valeurs critiques obtenues pour delta_0 =", delta0, ":\n")
  print(quantiles)
}


simulation_cst(delta0 = 5)
simulation_cst(delta0 = 10)
```

La valeur de la constante $\delta_0$ n'a pas d'impact significatif sur les résultats du test de Dickey-Fuller dans ce contexte. Peu importe si la constante est 5, 10, ou une autre valeur, la distribution des statistiques de test reste la même. Cela suggère que le test est robuste vis-à-vis de la valeur de la constante, et qu'il est davantage influencé par les autres caractéristiques du modèle plutôt que par la valeur spécifique de cette constante.

\newpage

# Exercice 5 : Analyse de la série temporelle du PIB des USA sur la période 1990-2023

## Import des données

```{r}
load("data/GDP-US-1990-2023.RData")
```

## Création des TimeSeries 

### Période globale (1990-2023)

```{r}
pib_pg <- ts(df_Y_us$PIB, start = c(1990, 1), frequency = 4)  
```

### Sous période 1990-2019

```{r}
pib_sp <- window(pib_pg, start = c(1990, 1), end = c(2019, 4))
```

## Question 1. Visualisation de la série

### Représentation graphique

```{r}
pib_sp_df <- data.frame(date = seq(from =
              as.Date("1990-01-01"),
              by = "quarter", 
              length.out = length(pib_sp)),
              PIB = as.numeric(pib_sp))

recessions <- data.frame(
  xmin = as.Date(c("1990-07-01", "2001-03-01", "2007-12-01")),
  xmax = as.Date(c("1991-03-01", "2001-11-01", "2009-06-01"))
)

ggplot(pib_sp_df, aes(x = date, y = PIB)) +
  geom_line(color = "darkred", linewidth = 1) + 
  geom_rect(data = recessions, 
            aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
            fill = "darkgray", 
            alpha = 0.5, 
            inherit.aes = FALSE) +
  scale_x_date(breaks = seq(as.Date("1990-01-01"), 
                            as.Date("2019-12-31"), 
                            by = "4 years"), 
               labels = date_format("%Y")) +
  labs(title = "Évolution du PIB US (1990-2019) avec récessions",
       x = "Année", y = "PIB (Milliards $)") +
  theme_minimal()
```

### Interprétation

Les périodes de récession ont été obtenues via le NBER (National Bureau of Economic Research).  
Nous retrouvons les périodes suivantes :

- Juillet 1990 - Mars 1991

- Mars 2001 - Novembre 2001

- Décembre 2007 - Juin 2009

**Récession de 1990-1991 (juillet 1990 - mars 1991)**

- Crise de l'épargne et des prêts (S&L Crisis) : plus de 1 000 institutions bancaires en faillite.

- Hausse des taux d'intérêt : la Fed resserre sa politique pour lutter contre l'inflation.

- Guerre du Golfe : l’invasion du Koweït fait bondir les prix du pétrole.

- Chômage : passe de 5,2 % en 1989 à 7,8 % en 1992.

**Récession de 2001 (mars 2001 - novembre 2001)**

- Éclatement de la bulle Internet (Dot-com crash) : chute massive des actions technologiques.

- Réduction des investissements : les entreprises limitent leurs dépenses.

- Attentats du 11 septembre : impact majeur sur la confiance économique.

- Chômage : passe de 4 % en 2000 à 6 % en 2003.

**Grande Récession de 2007-2009 (décembre 2007 - juin 2009)**

- Crise des subprimes : effondrement du marché immobilier.

- Faillite de Lehman Brothers (septembre 2008) : panique financière mondiale.

- Chute drastique du PIB : -4,3 %, pire contraction depuis la Seconde Guerre mondiale.

- Chômage : passe de 4,7 % en 2007 à 10 % en 2009.

- Intervention massive de la Fed : taux proches de 0 % et politique de Quantitative Easing (QE).

**Conclusion**

- 1990-1991 : crise bancaire et choc pétrolier.

- 2001 : bulle Internet et 11 septembre.

- 2007-2009 : crise financière et faillites bancaires.

**Résilience du PIB**

Malgré ces crises, l'économie américaine a toujours fini par rebondir.

## Question 2. ACF & PACF

## ACF

```{r}
Acf(pib_sp, main = "ACF du PIB US - 1990 à 2019")
```

La décroissance de l'ACF est lente, ce qui suggère la présence d'une racine unitaire.

### PACF

```{r}
Pacf(pib_sp, main = "PACF du PIB US - 1990 à 2019")
```


En ce qui concerne le PACF, nous constatons qu'excepté pour le premier lag, il n'y a aucun autre retard significatif.

## Question 3. Test de racine unitaire avec Dickey-Fuller

Pour effectuer les tests de Dickey-Fuller et Dickey-Fuller augmenté, nous allons utiliser la méthode séquentielle de Perron qui nous permettra de choisir le nombre de retards dans le test de racine unitaire et de déterminer si une tendance ou une constante doit être incluse dans le modèle.

#### Test de Dickey-Fuller simple 

Pour commencer, nous testerons le modèle numéro 3, avec la présence d'une tendance et d'une constante.

```{r}
adf_sp_trend <- ur.df(pib_sp, type = "trend", lags = 0)
summary(adf_sp_trend)
```

Nous constatons que les valeurs du t-stats sont largements supérieures en ce qui concerne tau3 et phi2, mais 1.27 est inférieur aux valeurs critiques pour phi3.
Alors, on ne rejette pas l'hypothèse de racine unitaire, la série est probablement non stationnaire, et il y a une tendance déterministe. Cependant, la tendance et la constante ne sont pas significatives ensemble.

Ensuite, nous allons donc tester le modèle 2, avec seulement une constante.

```{r}
adf_sp_drift <- ur.df(pib_sp, type = "drift", lags = 0)
summary(adf_sp_drift)
```

Ici, les t-stat sont biens significatifs et supérieurs aux valeurs critiques. Alors, nous pouvons déduire que la série est bien non stationnaire et qu'elle contient bien une tendance.

Nous effectuerons pour vérifier le test du modèle 1, sans tendance ni constante.

```{r}
adf_sp <- ur.df(pib_sp, type = "none", lags = 0)
summary(adf_sp)
```

Le t-stat est bien supérieur aux valeurs critiques. Ainsi, cela confirme la présence d'une racine unitaire et la non stationnarité de la série.

#### Test de Dickey-Fuller augmenté

Nous commençons de la même façon que pour le test simple par tester le modèle 3, avec tendance et terme constant.
Pour le choix du nombre de lag, nous ferons un choix automatique qui minimise l'AIC.

```{r}
adf_sp_trend <- ur.df(pib_sp, type = "trend", selectlags = "AIC")
summary(adf_sp_trend)
```

Dans ce cas, nous observons que les valeurs t-stat sont bien supérieures aux valeurs critiques pour tau3 et phi2, cependant, ce n'est pas le cas pour phi3. Alors, nous allons tester en enlevant la tendance.

```{r}
adf_sp_drift <- ur.df(pib_sp, type = "drift", selectlags = "AIC")
summary(adf_sp_drift)
```

Enfin, nous testerons le modèle sans tendance ni constante.

```{r}
adf_sp <- ur.df(pib_sp, type = "none", selectlags = "AIC")
summary(adf_sp)
```

La statistique de test est de 5.205, ce qui est largement supérieure aux valeurs critiques.
Alors, nous ne pouvons pas rejetter l’hypothèse de racine unitaire.
Ici, le t-stat étant très positif, la série est fortement non stationnaire, avec une présence de racine unitaire.

Pour conclure, d'après le test de Dickey Fuller simple et augmenté, effectués d'après la méthode séquentielle de Perron, nous trouvons que la série présente une tendance, est non stationnaire et contient une racine unitaire.

## Question 4. Test de stationnarité de KPSS

### Test et interprétation

```{r}
kpss_sp <- ur.kpss(pib_sp, type = "mu")
summary(kpss_sp)
```

La statistique de test (2.4528) est largement supérieure aux valeurs critiques à tous les seuils (1%, 5%, 10%).
Alors, on rejette l'hypothèse nulle de la stationnarité de la série avec une très forte certitude.
La série n'est donc pas stationnaire et contient une racine unitaire.

### Conclusion

D'après l'analyse effectuée ci-dessus, nous pouvons conclure que sur la période de 1990 à 2019, la série du PIB des US n'est pas stationnaire et comprend une racine unitaire. 
Ainsi, il faudra différencier la série pour la rendre stationnaire.
De plus, elle présente une certaine tendance.

## Question 5. Analyse sur période globale

### Question 1. Visualisation de la série

#### Représentation graphique

```{r}
pib_pg_df <- data.frame(date = seq(from =
              as.Date("1990-01-01"),
              by = "quarter", 
              length.out = length(pib_pg)),
              PIB = as.numeric(pib_pg))

recessions <- data.frame(
  xmin = as.Date(c("1990-07-01", "2001-03-01", "2007-12-01", "2020-02-01")),
  xmax = as.Date(c("1991-03-01", "2001-11-01", "2009-06-01", "2020-04-01"))
)

ggplot(pib_pg_df, aes(x = date, y = PIB)) +
  geom_line(color = "darkviolet", linewidth = 1) + 
  geom_rect(data = recessions, 
            aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
            fill = "darkgray", 
            alpha = 0.5, 
            inherit.aes = FALSE) +
  scale_x_date(breaks = seq(as.Date("1990-01-01"), 
                            as.Date("2023-12-31"), 
                            by = "3 years"), 
               labels = date_format("%Y")) +
  labs(title = "Évolution du PIB US (1990-2023) avec récessions",
       x = "Année", y = "PIB (Milliards $)") +
  theme_minimal()
```

#### Interprétation

**Récession de 2020 (février 2020 - avril 2020)**

En plus des récessions précédentes, on observe une forte récession mais très courte en 2020.  
Cette récession a été causée par la pandémie de COVID-19, qui a entraîné :

- Un arrêt brutal de l'économie dû aux confinements et restrictions sanitaires.

- Une chute massive de la consommation et des investissements.

- Une explosion du chômage, avec plus de 20 millions d'emplois perdus en quelques semaines.

- Une réponse économique rapide avec des mesures de soutien exceptionnelles, dont des aides directes aux ménages et entreprises.

**Conclusion**

- 1990-1991 : crise bancaire et choc pétrolier.

- 2001 : bulle Internet et 11 septembre.

- 2007-2009 : crise financière et faillites bancaires.

- 2020 : crise sanitaire et arrêt de l'économie.

Malgré ces chocs, l'économie américaine a toujours montré une capacité de rebond rapide.

### Question 2. ACF & PACF

```{r}
Acf(pib_pg, main = "ACF du PIB US - 1990 à 2023")
Pacf(pib_pg, main = "PACF du PIB US - 1990 à 2023")
```

Nous n'observons aucune différence significative par rapport aux autocorrélogrammes de la série représentant la sous-période allant jusqu'à 2019.

### Question 3. Test de racine unitaire avec Dickey-Fuller

#### Test de Dickey-Fuller simple 

Tout d'abord, nous testerons le modèle 3, avec la présence d'une tendance et d'une constante.

```{r}
adf_pg_trend <- ur.df(pib_pg, type = "trend", lags = 0)
summary(adf_pg_trend)
```

De la même façon que pour la série sur la sous période, nous voyons que les valeurs des t-stats sont supérieures en ce qui concerne tau3 et phi2, mais pas pour phi3.
Nous pouvons alors déduire qu'on ne rejette pas l'hypothèse de racine unitaire, la série est probablement non stationnaire, et il y a une tendance déterministe. D'autre part, la tendance et la constante ne sont pas significatives ensemble.

Puis, nous testerons le modèle 2, avec une constante mais sans tendance.

```{r}
adf_pg_drift <- ur.df(pib_pg, type = "drift", lags = 0)
summary(adf_pg_drift)
```

Ici aussi, les t-stat sont biens supérieurs aux valeurs critiques ; la série est non stationnaire et contient bien une tendance.

Enfin, nous vérifierons cela avec le test du modèle 1, sans tendance ni constante.

```{r}
adf_pg <- ur.df(pib_pg, type = "none", lags = 0)
summary(adf_pg)
```

Le t-stat est bien supérieur aux valeurs critiques. Nous trouvons donc bien la présence d'une racine unitaire et la non stationnarité de la série.

#### Test de Dickey-Fuller augmenté

Nous passons à présent au test de Dickey-Fuller augmenté.

```{r}
adf_pg_trend <- ur.df(pib_pg, type = "trend", selectlags = "AIC")
summary(adf_pg_trend)
```

Les valeurs t-stat sont bien supérieures aux valeurs critiques pour tau3 et phi2, mais pas pour phi3. 

Après cela, nous testerons le modèle 2.

```{r}
adf_pg_drift <- ur.df(pib_pg, type = "drift", selectlags = "AIC")
summary(adf_pg_drift)
```

Nous finirons avec le test du modèle 1.

```{r}
adf_pg <- ur.df(pib_pg, type = "none", selectlags = "AIC")
summary(adf_pg)
```

Pour résumé, d'après ces tests de Dickey Fuller, simple et augmenté, nous constatons que comme pour la sous période, cette série présente une tendance, est non stationnaire et contient une racine unitaire.

### Question 4. Test de stationnarité de KPSS

#### Test

```{r}
kpss_pg <- ur.kpss(pib_pg, type = "mu")
summary(kpss_pg)
```

Ici aussi, la statistique de test (2.8352) est largement supérieure aux valeurs critiques des différents seuils.
Nous rejeterons donc l'hypothèse nulle de la stationnarité de la série.
Comme pour la sous période analysée précedemment, la série n'est donc pas stationnaire et contient une racine unitaire.

#### Conclusion

Pour finir, nous pouvons conclure que peu importe le fait que la période étudiée contienne la récéssion du Covid ou non, les résultats et interprétations de l'analyse de la série restent les mêmes. Nous pouvons alors convenir que cette série est non stationnaire, contient une racine unitaire, et présente une tendance observable.